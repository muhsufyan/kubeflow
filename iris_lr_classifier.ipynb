{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjMWNUX7JLp9z6maRO7+Ty"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kfp"
      ],
      "metadata": {
        "id": "iMBYVyfscghG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX675GKYcSJK"
      },
      "outputs": [],
      "source": [
        "import kfp\n",
        "import kfp.components as comp\n",
        "import requests\n",
        "import kfp.dsl as dsl\n",
        "\n",
        "def prepare_data():\n",
        "  import pandas as pd\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/iris-data.csv\")\n",
        "  df = df.dropna()\n",
        "  df.to_csv(\"data/final_df.csv\", index=False)\n",
        "\n",
        "def split_data():\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  data = pd.read_csv(\"data/final_df.csv\")\n",
        "  target_feature = \"class\"\n",
        "  X = data.loc[:, data.columns != target_feature]\n",
        "  y = data.loc[:, data.columns == target_feature]\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=47)\n",
        "  print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "  np.save('data/x_train.npy', x_train)\n",
        "  np.save('data/x_test.npy', x_test)\n",
        "  np.save('data/y_train.npy', y_train)\n",
        "  np.save('data/y_test.npy', y_test)\n",
        "\n",
        "def training():\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "  x_train = np.load('data/x_train.npy', allow_pickle=True)\n",
        "  y_train = np.load('data/y_train.npy', allow_pickle=True)\n",
        "\n",
        "  classifier = LogisticRegression(max_iter=500)\n",
        "  classifier.fit(x_train,y_train)\n",
        "\n",
        "  import pickle\n",
        "  with open('data/model.pkl','wb') as f:\n",
        "    pickle.dump(classifier, f)\n",
        "\n",
        "def predict_test_data():\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import pickle\n",
        "  \n",
        "  with open('data/model.pkl','rb') as f:\n",
        "    lr_model = pickle.load(f)\n",
        "  \n",
        "  x_test = np.load('data/x_test.npy', allow_pickle = True)\n",
        "\n",
        "  y_pred = lr_model.predict(x_test)\n",
        "  print(y_pred)\n",
        "\n",
        "  np.save('data/y_predict_model.npy', y_pred)\n",
        "\n",
        "def predict_proba_test_data():\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import pickle\n",
        "  \n",
        "  with open('data/model.pkl','rb') as f:\n",
        "    lr_model = pickle.load(f)\n",
        "  \n",
        "  x_test = np.load('data/x_test.npy', allow_pickle = True)\n",
        "\n",
        "  y_pred_proba = lr_model.predict_proba(x_test)\n",
        "  print(y_pred_proba)\n",
        "\n",
        "  np.save('data/y_predict_proba_model.npy', y_pred_proba)\n",
        "\n",
        "def metrics():\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  from sklearn import metrics\n",
        "  from sklearn.metrics import accuracy_score, precision_score, recall_score, log_loss, roc_auc_score\n",
        "\n",
        "  y_test = np.load('data/y_test.npy', allow_pickle=True)\n",
        "  y_pred = np.load('data/y_predict_model.npy', allow_pickle=True)\n",
        "  y_pred_proba = np.load('data/y_predict_proba_model.npy', allow_pickle=True)\n",
        "\n",
        "  acc = accuracy_score(y_test,y_pred)\n",
        "  recall = recall_score(y_test, y_pred, average='micro')\n",
        "  prec = precision_score(y_test, y_pred, average='micro')\n",
        "  entropy = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "  print(\"\\nMetrics :\", {\"accuracy\": round(acc,2), \"recall\": round(recall,2), \"precision\": round(prec,2), \"log loss\": round(entropy,2)})\n",
        "\n",
        "  print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "create_component_prepare_data = kfp.components.create_component_from_func(\n",
        "    func = prepare_data,\n",
        "    base_image = 'python:3.8.10',\n",
        "    packages_to_install = ['pandas == 1.3.5','numpy == 1.22.4'])\n",
        "create_component_split_data = kfp.components.create_component_from_func(\n",
        "    func = split_data,\n",
        "    base_image = 'python:3.8.10',\n",
        "    packages_to_install = ['pandas == 1.3.5','numpy == 1.22.4','scikit-learn == 1.2.1']\n",
        ")\n",
        "create_component_training = kfp.components.create_component_from_func(\n",
        "    func = training,\n",
        "    base_image = 'python:3.8.10',\n",
        "    packages_to_install = ['pandas == 1.3.5','numpy == 1.22.4','scikit-learn == 1.2.1']\n",
        ")\n",
        "create_component_predict_test_data = kfp.components.create_component_from_func(\n",
        "    func = predict_test_data,\n",
        "    base_image = 'python:3.8.10',\n",
        "    packages_to_install = ['pandas == 1.3.5','numpy == 1.22.4','scikit-learn == 1.2.1']\n",
        ")\n",
        "create_component_predict_proba_test_data = kfp.components.create_component_from_func(\n",
        "    func = predict_proba_test_data,\n",
        "    base_image = 'python:3.8.10',\n",
        "    packages_to_install = ['pandas == 1.3.5','numpy == 1.22.4','scikit-learn == 1.2.1']\n",
        ")\n",
        "create_component_metrics = kfp.components.create_component_from_func(\n",
        "    func = metrics,\n",
        "    base_image = 'python:3.8.10',\n",
        "    packages_to_install = ['pandas == 1.3.5','numpy == 1.22.4','scikit-learn == 1.2.1']\n",
        ")\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name = \" Logistic Regression model classifier for IRIS dataset\",\n",
        "    description = \"simple kubeflow pipeline\"\n",
        ")\n",
        "\n",
        "def iris_lr_classifier(data_path: str):\n",
        "  vop = dsl.VolumeOp(\n",
        "      name = \"t-vol\",\n",
        "      resource_name = 't-vol',\n",
        "      size = '200M',\n",
        "      modes = dsl.VOLUME_MODE_RWO)\n",
        "  \n",
        "  prepare_data_task = create_component_prepare_data.add_pvolumes({data_path: vop.volume})\n",
        "  split_task = create_component_split_data.add_pvolumes({data_path: vop.volume}).after(prepare_data_task)\n",
        "  classifier_training = create_component_training.add_pvolumes({data_path: vop.volume}).after(split_task)\n",
        "  log_predicted_class = create_component_predict_test_data.add_pvolumes({data_path: vop.volume}).after(classifier_training)\n",
        "  log_predicted_probabilities = create_component_predict_proba_test_data.add_pvolumes({data_path: vop.volume}).after(log_predicted_class)\n",
        "  log_metrics_task = create_component_metrics.add_pvolumes({data_path: vop.volume}).after(log_predicted_probabilities)\n",
        "\n",
        "\n",
        "  prepare_data_task.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
        "  split_task.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
        "  classifier_training.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
        "  log_predicted_class.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
        "  log_predicted_probabilities.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
        "  log_metrics_task.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
        "\n",
        "kfp.compiler.Compiler.compile(pipeline_func=iris_lr_classifier,\n",
        "                              package_path='data/lr_iris_classifier_pipeline1.yaml'\n",
        ")\n",
        "\n",
        "cleint = kfp.Client()\n",
        "\n",
        "DATA_PATH = '/data'\n",
        "\n",
        "import datetime\n",
        "print(datetime.datetime.now().date())\n",
        "\n",
        "\n",
        "pipeline_func = iris_lr_classifier\n",
        "\n",
        "experiment_name = 'iris_classifier_experiment_'+ str(datetime.datetime.now().date())\n",
        "\n",
        "run_name = pipeline_func.__name__+' run'\n",
        "\n",
        "namespace = 'kubeflow'\n",
        "\n",
        "arguments = {\"data path\": DATA_PATH}\n",
        "\n",
        "kfp.compiler.Compiler.compile(\n",
        "    pipeline_func,\n",
        "    '{}.zip'.format(experiment_name))\n",
        "\n",
        "run_result = client.create_run_from_pipeline_func(pipeline_func,\n",
        "                                                  experiment_name = experiment_name,\n",
        "                                                  run_name = run_name,\n",
        "                                                  arguments = arguments)\n"
      ]
    }
  ]
}